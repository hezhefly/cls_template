2021-05-28 13:04:54 | INFO | DotMap(_name_='swin.yaml', model=DotMap(in_channel=3, num_classes=3, model_name='swin', resume=True, weights='assets/weights/swin_tiny_patch4_window7_224.pt'), train=DotMap(batch_size=24, max_epoch=30), test=DotMap(batch_size=24), solver=DotMap(seed=42, device='cuda:0', lr=0.001), datasets=DotMap(images_dir='data/images', train_file='data/train.txt', val_file='data/val.txt'), save=DotMap(log_dir='assets/logs', weight_dir='assets/weights', val_per_epoch=2))
2021-05-28 13:04:56 | INFO | 240 train data loaded,batch size: 24, image size:224
2021-05-28 13:04:56 | INFO | 60 val data loaded,batch size: 24
2021-05-28 13:04:56 | INFO | load layers: dict_keys(['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'layers.0.blocks.0.norm1.weight', 'layers.0.blocks.0.norm1.bias', 'layers.0.blocks.0.attn.qkv.weight', 'layers.0.blocks.0.attn.qkv.bias', 'layers.0.blocks.0.attn.proj.weight', 'layers.0.blocks.0.attn.proj.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.weight', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.weight', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm1.weight', 'layers.0.blocks.1.norm1.bias', 'layers.0.blocks.1.attn.qkv.weight', 'layers.0.blocks.1.attn.qkv.bias', 'layers.0.blocks.1.attn.proj.weight', 'layers.0.blocks.1.attn.proj.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.weight', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.weight', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.norm.weight', 'layers.0.downsample.norm.bias', 'layers.1.blocks.0.norm1.weight', 'layers.1.blocks.0.norm1.bias', 'layers.1.blocks.0.attn.qkv.weight', 'layers.1.blocks.0.attn.qkv.bias', 'layers.1.blocks.0.attn.proj.weight', 'layers.1.blocks.0.attn.proj.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.weight', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.weight', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm1.weight', 'layers.1.blocks.1.norm1.bias', 'layers.1.blocks.1.attn.qkv.weight', 'layers.1.blocks.1.attn.qkv.bias', 'layers.1.blocks.1.attn.proj.weight', 'layers.1.blocks.1.attn.proj.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.weight', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.weight', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.norm.weight', 'layers.1.downsample.norm.bias', 'layers.2.blocks.0.norm1.weight', 'layers.2.blocks.0.norm1.bias', 'layers.2.blocks.0.attn.qkv.weight', 'layers.2.blocks.0.attn.qkv.bias', 'layers.2.blocks.0.attn.proj.weight', 'layers.2.blocks.0.attn.proj.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.weight', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.weight', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm1.weight', 'layers.2.blocks.1.norm1.bias', 'layers.2.blocks.1.attn.qkv.weight', 'layers.2.blocks.1.attn.qkv.bias', 'layers.2.blocks.1.attn.proj.weight', 'layers.2.blocks.1.attn.proj.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.weight', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.weight', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.qkv.bias', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.qkv.bias', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.qkv.bias', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.qkv.bias', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.downsample.norm.weight', 'layers.2.downsample.norm.bias', 'layers.3.blocks.0.norm1.weight', 'layers.3.blocks.0.norm1.bias', 'layers.3.blocks.0.attn.qkv.weight', 'layers.3.blocks.0.attn.qkv.bias', 'layers.3.blocks.0.attn.proj.weight', 'layers.3.blocks.0.attn.proj.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.weight', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.weight', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm1.weight', 'layers.3.blocks.1.norm1.bias', 'layers.3.blocks.1.attn.qkv.weight', 'layers.3.blocks.1.attn.qkv.bias', 'layers.3.blocks.1.attn.proj.weight', 'layers.3.blocks.1.attn.proj.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.weight', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.weight', 'layers.3.blocks.1.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.1.blocks.1.attn_mask', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.5.attn_mask', 'layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.0.downsample.reduction.weight', 'layers.1.downsample.reduction.weight', 'layers.2.downsample.reduction.weight'])
2021-05-28 13:04:58 | INFO | Train Epoch: [0/30] [24/240], current batch Loss: 1.111219 lr: 0.001
2021-05-28 13:04:58 | INFO | Train Epoch: [0/30] [48/240], current batch Loss: 0.999839 lr: 0.001
2021-05-28 13:04:59 | INFO | Train Epoch: [0/30] [72/240], current batch Loss: 1.194461 lr: 0.001
2021-05-28 13:04:59 | INFO | Train Epoch: [0/30] [96/240], current batch Loss: 1.094860 lr: 0.001
2021-05-28 13:05:00 | INFO | Train Epoch: [0/30] [120/240], current batch Loss: 0.983198 lr: 0.001
2021-05-28 13:05:00 | INFO | Train Epoch: [0/30] [144/240], current batch Loss: 1.126171 lr: 0.001
2021-05-28 13:05:00 | INFO | Train Epoch: [0/30] [168/240], current batch Loss: 1.048906 lr: 0.001
2021-05-28 13:05:01 | INFO | Train Epoch: [0/30] [192/240], current batch Loss: 1.016719 lr: 0.001
2021-05-28 13:05:01 | INFO | Train Epoch: [0/30] [216/240], current batch Loss: 1.037958 lr: 0.001
2021-05-28 13:05:02 | INFO | Train Epoch: [0/30] [240/240], current batch Loss: 1.030591 lr: 0.001
2021-05-28 13:05:02 | INFO | Train Epoch: [1/30] [24/240], current batch Loss: 1.069769 lr: 0.001
2021-05-28 13:05:03 | INFO | Train Epoch: [1/30] [48/240], current batch Loss: 0.985563 lr: 0.001
2021-05-28 13:05:03 | INFO | Train Epoch: [1/30] [72/240], current batch Loss: 1.023530 lr: 0.001
2021-05-28 13:05:04 | INFO | Train Epoch: [1/30] [96/240], current batch Loss: 0.882658 lr: 0.001
2021-05-28 13:05:04 | INFO | Train Epoch: [1/30] [120/240], current batch Loss: 0.949813 lr: 0.001
2021-05-28 13:05:05 | INFO | Train Epoch: [1/30] [144/240], current batch Loss: 0.922763 lr: 0.001
2021-05-28 13:05:05 | INFO | Train Epoch: [1/30] [168/240], current batch Loss: 0.918630 lr: 0.001
2021-05-28 13:05:06 | INFO | Train Epoch: [1/30] [192/240], current batch Loss: 0.967731 lr: 0.001
2021-05-28 13:05:06 | INFO | Train Epoch: [1/30] [216/240], current batch Loss: 1.136265 lr: 0.001
2021-05-28 13:05:06 | INFO | Train Epoch: [1/30] [240/240], current batch Loss: 0.888762 lr: 0.001
2021-05-28 13:05:07 | INFO | val loss:0.9307457327842712, acc: 68.33333053588868
2021-05-28 13:05:08 | INFO | Train Epoch: [2/30] [24/240], current batch Loss: 0.945912 lr: 0.001
2021-05-28 13:05:08 | INFO | Train Epoch: [2/30] [48/240], current batch Loss: 0.907512 lr: 0.001
2021-05-28 13:05:08 | INFO | Train Epoch: [2/30] [72/240], current batch Loss: 0.933224 lr: 0.001
2021-05-28 13:05:09 | INFO | Train Epoch: [2/30] [96/240], current batch Loss: 0.959488 lr: 0.001
2021-05-28 13:05:09 | INFO | Train Epoch: [2/30] [120/240], current batch Loss: 0.870876 lr: 0.001
2021-05-28 13:05:10 | INFO | Train Epoch: [2/30] [144/240], current batch Loss: 0.952143 lr: 0.001
2021-05-28 13:05:10 | INFO | Train Epoch: [2/30] [168/240], current batch Loss: 0.853998 lr: 0.001
2021-05-28 13:05:11 | INFO | Train Epoch: [2/30] [192/240], current batch Loss: 0.868643 lr: 0.001
2021-05-28 13:05:11 | INFO | Train Epoch: [2/30] [216/240], current batch Loss: 0.842257 lr: 0.001
2021-05-28 13:05:12 | INFO | Train Epoch: [2/30] [240/240], current batch Loss: 0.803891 lr: 0.001
2021-05-28 13:05:12 | INFO | Train Epoch: [3/30] [24/240], current batch Loss: 0.816229 lr: 0.001
2021-05-28 13:05:13 | INFO | Train Epoch: [3/30] [48/240], current batch Loss: 0.902016 lr: 0.001
2021-05-28 13:05:13 | INFO | Train Epoch: [3/30] [72/240], current batch Loss: 0.847494 lr: 0.001
2021-05-28 13:05:14 | INFO | Train Epoch: [3/30] [96/240], current batch Loss: 0.804097 lr: 0.001
2021-05-28 13:05:14 | INFO | Train Epoch: [3/30] [120/240], current batch Loss: 0.923843 lr: 0.001
2021-05-28 13:05:14 | INFO | Train Epoch: [3/30] [144/240], current batch Loss: 0.787996 lr: 0.001
2021-05-28 13:05:15 | INFO | Train Epoch: [3/30] [168/240], current batch Loss: 0.809186 lr: 0.001
2021-05-28 13:05:15 | INFO | Train Epoch: [3/30] [192/240], current batch Loss: 0.796335 lr: 0.001
2021-05-28 13:05:16 | INFO | Train Epoch: [3/30] [216/240], current batch Loss: 0.782977 lr: 0.001
2021-05-28 13:05:16 | INFO | Train Epoch: [3/30] [240/240], current batch Loss: 0.734723 lr: 0.001
2021-05-28 13:05:17 | INFO | val loss:0.7747031688690186, acc: 84.9999984741211
2021-05-28 13:05:18 | INFO | Train Epoch: [4/30] [24/240], current batch Loss: 0.795286 lr: 0.001
2021-05-28 13:05:18 | INFO | Train Epoch: [4/30] [48/240], current batch Loss: 0.703552 lr: 0.001
2021-05-28 13:05:18 | INFO | Train Epoch: [4/30] [72/240], current batch Loss: 0.682203 lr: 0.001
2021-05-28 13:05:19 | INFO | Train Epoch: [4/30] [96/240], current batch Loss: 0.770005 lr: 0.001
2021-05-28 13:05:19 | INFO | Train Epoch: [4/30] [120/240], current batch Loss: 0.815429 lr: 0.001
2021-05-28 13:05:20 | INFO | Train Epoch: [4/30] [144/240], current batch Loss: 0.701993 lr: 0.001
2021-05-28 13:05:20 | INFO | Train Epoch: [4/30] [168/240], current batch Loss: 0.666175 lr: 0.001
2021-05-28 13:05:21 | INFO | Train Epoch: [4/30] [192/240], current batch Loss: 0.761235 lr: 0.001
2021-05-28 13:05:21 | INFO | Train Epoch: [4/30] [216/240], current batch Loss: 0.724199 lr: 0.001
2021-05-28 13:05:22 | INFO | Train Epoch: [4/30] [240/240], current batch Loss: 0.726380 lr: 0.001
2021-05-28 13:05:22 | INFO | Train Epoch: [5/30] [24/240], current batch Loss: 0.701575 lr: 0.001
2021-05-28 13:05:23 | INFO | Train Epoch: [5/30] [48/240], current batch Loss: 0.699282 lr: 0.001
2021-05-28 13:05:23 | INFO | Train Epoch: [5/30] [72/240], current batch Loss: 0.690652 lr: 0.001
2021-05-28 13:05:24 | INFO | Train Epoch: [5/30] [96/240], current batch Loss: 0.816758 lr: 0.001
2021-05-28 13:05:24 | INFO | Train Epoch: [5/30] [120/240], current batch Loss: 0.690637 lr: 0.001
2021-05-28 13:05:25 | INFO | Train Epoch: [5/30] [144/240], current batch Loss: 0.730937 lr: 0.001
2021-05-28 13:05:25 | INFO | Train Epoch: [5/30] [168/240], current batch Loss: 0.689800 lr: 0.001
2021-05-28 13:05:25 | INFO | Train Epoch: [5/30] [192/240], current batch Loss: 0.643667 lr: 0.001
2021-05-28 13:05:26 | INFO | Train Epoch: [5/30] [216/240], current batch Loss: 0.682362 lr: 0.001
2021-05-28 13:05:26 | INFO | Train Epoch: [5/30] [240/240], current batch Loss: 0.592063 lr: 0.001
2021-05-28 13:05:27 | INFO | val loss:0.6517431378364563, acc: 89.9999984741211
2021-05-28 13:05:28 | INFO | Train Epoch: [6/30] [24/240], current batch Loss: 0.638425 lr: 0.001
2021-05-28 13:05:28 | INFO | Train Epoch: [6/30] [48/240], current batch Loss: 0.705639 lr: 0.001
2021-05-28 13:05:29 | INFO | Train Epoch: [6/30] [72/240], current batch Loss: 0.685525 lr: 0.001
2021-05-28 13:05:29 | INFO | Train Epoch: [6/30] [96/240], current batch Loss: 0.616186 lr: 0.001
2021-05-28 13:05:30 | INFO | Train Epoch: [6/30] [120/240], current batch Loss: 0.720308 lr: 0.001
2021-05-28 13:05:30 | INFO | Train Epoch: [6/30] [144/240], current batch Loss: 0.632394 lr: 0.001
2021-05-28 13:05:31 | INFO | Train Epoch: [6/30] [168/240], current batch Loss: 0.593605 lr: 0.001
2021-05-28 13:05:31 | INFO | Train Epoch: [6/30] [192/240], current batch Loss: 0.672456 lr: 0.001
2021-05-28 13:05:31 | INFO | Train Epoch: [6/30] [216/240], current batch Loss: 0.582027 lr: 0.001
2021-05-28 13:05:32 | INFO | Train Epoch: [6/30] [240/240], current batch Loss: 0.650518 lr: 0.001
2021-05-28 13:05:32 | INFO | Train Epoch: [7/30] [24/240], current batch Loss: 0.571184 lr: 0.001
2021-05-28 13:05:33 | INFO | Train Epoch: [7/30] [48/240], current batch Loss: 0.693902 lr: 0.001
2021-05-28 13:05:33 | INFO | Train Epoch: [7/30] [72/240], current batch Loss: 0.576650 lr: 0.001
2021-05-28 13:05:34 | INFO | Train Epoch: [7/30] [96/240], current batch Loss: 0.576366 lr: 0.001
2021-05-28 13:05:34 | INFO | Train Epoch: [7/30] [120/240], current batch Loss: 0.635267 lr: 0.001
2021-05-28 13:05:35 | INFO | Train Epoch: [7/30] [144/240], current batch Loss: 0.616091 lr: 0.001
2021-05-28 13:05:35 | INFO | Train Epoch: [7/30] [168/240], current batch Loss: 0.490423 lr: 0.001
2021-05-28 13:05:36 | INFO | Train Epoch: [7/30] [192/240], current batch Loss: 0.561425 lr: 0.001
2021-05-28 13:05:36 | INFO | Train Epoch: [7/30] [216/240], current batch Loss: 0.628564 lr: 0.001
2021-05-28 13:05:37 | INFO | Train Epoch: [7/30] [240/240], current batch Loss: 0.566880 lr: 0.001
2021-05-28 13:05:37 | INFO | val loss:0.5506563127040863, acc: 89.9999984741211
2021-05-28 13:05:38 | INFO | Train Epoch: [8/30] [24/240], current batch Loss: 0.559444 lr: 0.001
2021-05-28 13:05:38 | INFO | Train Epoch: [8/30] [48/240], current batch Loss: 0.528185 lr: 0.001
2021-05-28 13:05:38 | INFO | Train Epoch: [8/30] [72/240], current batch Loss: 0.485690 lr: 0.001
2021-05-28 13:05:39 | INFO | Train Epoch: [8/30] [96/240], current batch Loss: 0.581224 lr: 0.001
2021-05-28 13:05:39 | INFO | Train Epoch: [8/30] [120/240], current batch Loss: 0.529016 lr: 0.001
2021-05-28 13:05:40 | INFO | Train Epoch: [8/30] [144/240], current batch Loss: 0.557966 lr: 0.001
2021-05-28 13:05:40 | INFO | Train Epoch: [8/30] [168/240], current batch Loss: 0.543793 lr: 0.001
2021-05-28 13:05:41 | INFO | Train Epoch: [8/30] [192/240], current batch Loss: 0.463440 lr: 0.001
2021-05-28 13:05:41 | INFO | Train Epoch: [8/30] [216/240], current batch Loss: 0.536166 lr: 0.001
2021-05-28 13:05:42 | INFO | Train Epoch: [8/30] [240/240], current batch Loss: 0.600687 lr: 0.001
2021-05-28 13:05:42 | INFO | Train Epoch: [9/30] [24/240], current batch Loss: 0.429775 lr: 0.001
2021-05-28 13:05:43 | INFO | Train Epoch: [9/30] [48/240], current batch Loss: 0.448765 lr: 0.001
2021-05-28 13:05:43 | INFO | Train Epoch: [9/30] [72/240], current batch Loss: 0.497466 lr: 0.001
2021-05-28 13:05:44 | INFO | Train Epoch: [9/30] [96/240], current batch Loss: 0.481631 lr: 0.001
2021-05-28 13:05:44 | INFO | Train Epoch: [9/30] [120/240], current batch Loss: 0.521042 lr: 0.001
2021-05-28 13:05:45 | INFO | Train Epoch: [9/30] [144/240], current batch Loss: 0.517623 lr: 0.001
2021-05-28 13:05:45 | INFO | Train Epoch: [9/30] [168/240], current batch Loss: 0.439843 lr: 0.001
2021-05-28 13:05:46 | INFO | Train Epoch: [9/30] [192/240], current batch Loss: 0.560588 lr: 0.001
2021-05-28 13:05:46 | INFO | Train Epoch: [9/30] [216/240], current batch Loss: 0.539112 lr: 0.001
2021-05-28 13:05:46 | INFO | Train Epoch: [9/30] [240/240], current batch Loss: 0.444209 lr: 0.001
2021-05-28 13:05:47 | INFO | val loss:0.46961213946342467, acc: 95.0
2021-05-28 13:05:48 | INFO | Train Epoch: [10/30] [24/240], current batch Loss: 0.476428 lr: 0.001
2021-05-28 13:05:48 | INFO | Train Epoch: [10/30] [48/240], current batch Loss: 0.579028 lr: 0.001
2021-05-28 13:05:49 | INFO | Train Epoch: [10/30] [72/240], current batch Loss: 0.456855 lr: 0.001
2021-05-28 13:05:49 | INFO | Train Epoch: [10/30] [96/240], current batch Loss: 0.421874 lr: 0.001
2021-05-28 13:05:50 | INFO | Train Epoch: [10/30] [120/240], current batch Loss: 0.432537 lr: 0.001
2021-05-28 13:05:50 | INFO | Train Epoch: [10/30] [144/240], current batch Loss: 0.535341 lr: 0.001
2021-05-28 13:05:51 | INFO | Train Epoch: [10/30] [168/240], current batch Loss: 0.411814 lr: 0.001
2021-05-28 13:05:51 | INFO | Train Epoch: [10/30] [192/240], current batch Loss: 0.471757 lr: 0.001
2021-05-28 13:05:52 | INFO | Train Epoch: [10/30] [216/240], current batch Loss: 0.429852 lr: 0.001
2021-05-28 13:05:52 | INFO | Train Epoch: [10/30] [240/240], current batch Loss: 0.441888 lr: 0.001
2021-05-28 13:05:53 | INFO | Train Epoch: [11/30] [24/240], current batch Loss: 0.453080 lr: 0.001
2021-05-28 13:05:53 | INFO | Train Epoch: [11/30] [48/240], current batch Loss: 0.351208 lr: 0.001
2021-05-28 13:05:54 | INFO | Train Epoch: [11/30] [72/240], current batch Loss: 0.400075 lr: 0.001
2021-05-28 13:05:54 | INFO | Train Epoch: [11/30] [96/240], current batch Loss: 0.379419 lr: 0.001
2021-05-28 13:05:55 | INFO | Train Epoch: [11/30] [120/240], current batch Loss: 0.392331 lr: 0.001
2021-05-28 13:05:55 | INFO | Train Epoch: [11/30] [144/240], current batch Loss: 0.447709 lr: 0.001
2021-05-28 13:05:55 | INFO | Train Epoch: [11/30] [168/240], current batch Loss: 0.465969 lr: 0.001
2021-05-28 13:05:56 | INFO | Train Epoch: [11/30] [192/240], current batch Loss: 0.417071 lr: 0.001
2021-05-28 13:05:56 | INFO | Train Epoch: [11/30] [216/240], current batch Loss: 0.345598 lr: 0.001
2021-05-28 13:05:57 | INFO | Train Epoch: [11/30] [240/240], current batch Loss: 0.505457 lr: 0.001
2021-05-28 13:05:57 | INFO | val loss:0.40219739079475403, acc: 95.0
2021-05-28 13:05:58 | INFO | Train Epoch: [12/30] [24/240], current batch Loss: 0.395507 lr: 0.001
2021-05-28 13:05:58 | INFO | Train Epoch: [12/30] [48/240], current batch Loss: 0.421606 lr: 0.001
2021-05-28 13:05:59 | INFO | Train Epoch: [12/30] [72/240], current batch Loss: 0.403807 lr: 0.001
2021-05-28 13:06:00 | INFO | Train Epoch: [12/30] [96/240], current batch Loss: 0.346933 lr: 0.001
2021-05-28 13:06:00 | INFO | Train Epoch: [12/30] [120/240], current batch Loss: 0.383797 lr: 0.001
2021-05-28 13:06:00 | INFO | Train Epoch: [12/30] [144/240], current batch Loss: 0.374836 lr: 0.001
2021-05-28 13:06:01 | INFO | Train Epoch: [12/30] [168/240], current batch Loss: 0.405070 lr: 0.001
2021-05-28 13:06:01 | INFO | Train Epoch: [12/30] [192/240], current batch Loss: 0.418733 lr: 0.001
2021-05-28 13:06:02 | INFO | Train Epoch: [12/30] [216/240], current batch Loss: 0.356128 lr: 0.001
2021-05-28 13:06:02 | INFO | Train Epoch: [12/30] [240/240], current batch Loss: 0.430895 lr: 0.001
2021-05-28 13:06:03 | INFO | Train Epoch: [13/30] [24/240], current batch Loss: 0.326515 lr: 0.001
2021-05-28 13:06:03 | INFO | Train Epoch: [13/30] [48/240], current batch Loss: 0.449940 lr: 0.001
2021-05-28 13:06:04 | INFO | Train Epoch: [13/30] [72/240], current batch Loss: 0.333054 lr: 0.001
2021-05-28 13:06:04 | INFO | Train Epoch: [13/30] [96/240], current batch Loss: 0.419087 lr: 0.001
2021-05-28 13:06:05 | INFO | Train Epoch: [13/30] [120/240], current batch Loss: 0.322539 lr: 0.001
2021-05-28 13:06:05 | INFO | Train Epoch: [13/30] [144/240], current batch Loss: 0.313892 lr: 0.001
2021-05-28 13:06:06 | INFO | Train Epoch: [13/30] [168/240], current batch Loss: 0.398916 lr: 0.001
2021-05-28 13:06:06 | INFO | Train Epoch: [13/30] [192/240], current batch Loss: 0.278004 lr: 0.001
2021-05-28 13:06:07 | INFO | Train Epoch: [13/30] [216/240], current batch Loss: 0.359603 lr: 0.001
2021-05-28 13:06:07 | INFO | Train Epoch: [13/30] [240/240], current batch Loss: 0.401043 lr: 0.001
2021-05-28 13:06:08 | INFO | val loss:0.34570206999778746, acc: 95.0
2021-05-28 13:06:08 | INFO | Train Epoch: [14/30] [24/240], current batch Loss: 0.399211 lr: 0.001
2021-05-28 13:06:09 | INFO | Train Epoch: [14/30] [48/240], current batch Loss: 0.358361 lr: 0.001
2021-05-28 13:06:09 | INFO | Train Epoch: [14/30] [72/240], current batch Loss: 0.307812 lr: 0.001
2021-05-28 13:06:10 | INFO | Train Epoch: [14/30] [96/240], current batch Loss: 0.405750 lr: 0.001
2021-05-28 13:06:10 | INFO | Train Epoch: [14/30] [120/240], current batch Loss: 0.235105 lr: 0.001
2021-05-28 13:06:11 | INFO | Train Epoch: [14/30] [144/240], current batch Loss: 0.274173 lr: 0.001
2021-05-28 13:06:11 | INFO | Train Epoch: [14/30] [168/240], current batch Loss: 0.327911 lr: 0.001
2021-05-28 13:06:12 | INFO | Train Epoch: [14/30] [192/240], current batch Loss: 0.337665 lr: 0.001
2021-05-28 13:06:12 | INFO | Train Epoch: [14/30] [216/240], current batch Loss: 0.320876 lr: 0.001
2021-05-28 13:06:12 | INFO | Train Epoch: [14/30] [240/240], current batch Loss: 0.323314 lr: 0.001
2021-05-28 13:06:13 | INFO | Train Epoch: [15/30] [24/240], current batch Loss: 0.399764 lr: 0.001
2021-05-28 13:06:13 | INFO | Train Epoch: [15/30] [48/240], current batch Loss: 0.282352 lr: 0.001
2021-05-28 13:06:14 | INFO | Train Epoch: [15/30] [72/240], current batch Loss: 0.305067 lr: 0.001
2021-05-28 13:06:14 | INFO | Train Epoch: [15/30] [96/240], current batch Loss: 0.270812 lr: 0.001
2021-05-28 13:06:15 | INFO | Train Epoch: [15/30] [120/240], current batch Loss: 0.376294 lr: 0.001
2021-05-28 13:06:15 | INFO | Train Epoch: [15/30] [144/240], current batch Loss: 0.380744 lr: 0.001
2021-05-28 13:06:16 | INFO | Train Epoch: [15/30] [168/240], current batch Loss: 0.365017 lr: 0.001
2021-05-28 13:06:16 | INFO | Train Epoch: [15/30] [192/240], current batch Loss: 0.294104 lr: 0.001
2021-05-28 13:06:17 | INFO | Train Epoch: [15/30] [216/240], current batch Loss: 0.305513 lr: 0.001
2021-05-28 13:06:17 | INFO | Train Epoch: [15/30] [240/240], current batch Loss: 0.290526 lr: 0.001
2021-05-28 13:06:18 | INFO | val loss:0.29800203144550325, acc: 96.66666564941406
2021-05-28 13:06:18 | INFO | Train Epoch: [16/30] [24/240], current batch Loss: 0.256533 lr: 0.001
2021-05-28 13:06:19 | INFO | Train Epoch: [16/30] [48/240], current batch Loss: 0.233033 lr: 0.001
2021-05-28 13:06:20 | INFO | Train Epoch: [16/30] [72/240], current batch Loss: 0.221771 lr: 0.001
2021-05-28 13:06:20 | INFO | Train Epoch: [16/30] [96/240], current batch Loss: 0.240767 lr: 0.001
2021-05-28 13:06:20 | INFO | Train Epoch: [16/30] [120/240], current batch Loss: 0.321985 lr: 0.001
2021-05-28 13:06:21 | INFO | Train Epoch: [16/30] [144/240], current batch Loss: 0.353925 lr: 0.001
2021-05-28 13:06:21 | INFO | Train Epoch: [16/30] [168/240], current batch Loss: 0.239241 lr: 0.001
2021-05-28 13:06:22 | INFO | Train Epoch: [16/30] [192/240], current batch Loss: 0.313393 lr: 0.001
2021-05-28 13:06:22 | INFO | Train Epoch: [16/30] [216/240], current batch Loss: 0.292824 lr: 0.001
2021-05-28 13:06:23 | INFO | Train Epoch: [16/30] [240/240], current batch Loss: 0.235210 lr: 0.001
2021-05-28 13:06:23 | INFO | Train Epoch: [17/30] [24/240], current batch Loss: 0.312091 lr: 0.001
2021-05-28 13:06:24 | INFO | Train Epoch: [17/30] [48/240], current batch Loss: 0.339473 lr: 0.001
2021-05-28 13:06:24 | INFO | Train Epoch: [17/30] [72/240], current batch Loss: 0.259821 lr: 0.001
2021-05-28 13:06:25 | INFO | Train Epoch: [17/30] [96/240], current batch Loss: 0.170265 lr: 0.001
2021-05-28 13:06:25 | INFO | Train Epoch: [17/30] [120/240], current batch Loss: 0.229292 lr: 0.001
2021-05-28 13:06:26 | INFO | Train Epoch: [17/30] [144/240], current batch Loss: 0.230773 lr: 0.001
2021-05-28 13:06:26 | INFO | Train Epoch: [17/30] [168/240], current batch Loss: 0.281739 lr: 0.001
2021-05-28 13:06:27 | INFO | Train Epoch: [17/30] [192/240], current batch Loss: 0.320630 lr: 0.001
2021-05-28 13:06:27 | INFO | Train Epoch: [17/30] [216/240], current batch Loss: 0.263395 lr: 0.001
2021-05-28 13:06:28 | INFO | Train Epoch: [17/30] [240/240], current batch Loss: 0.281402 lr: 0.001
2021-05-28 13:06:28 | INFO | val loss:0.26666403710842135, acc: 96.66666564941406
2021-05-28 13:06:29 | INFO | Train Epoch: [18/30] [24/240], current batch Loss: 0.218830 lr: 0.001
2021-05-28 13:06:29 | INFO | Train Epoch: [18/30] [48/240], current batch Loss: 0.272110 lr: 0.001
2021-05-28 13:06:30 | INFO | Train Epoch: [18/30] [72/240], current batch Loss: 0.230039 lr: 0.001
2021-05-28 13:06:30 | INFO | Train Epoch: [18/30] [96/240], current batch Loss: 0.306685 lr: 0.001
2021-05-28 13:06:31 | INFO | Train Epoch: [18/30] [120/240], current batch Loss: 0.167871 lr: 0.001
2021-05-28 13:06:32 | INFO | Train Epoch: [18/30] [144/240], current batch Loss: 0.220879 lr: 0.001
2021-05-28 13:06:32 | INFO | Train Epoch: [18/30] [168/240], current batch Loss: 0.288449 lr: 0.001
2021-05-28 13:06:33 | INFO | Train Epoch: [18/30] [192/240], current batch Loss: 0.262980 lr: 0.001
2021-05-28 13:06:33 | INFO | Train Epoch: [18/30] [216/240], current batch Loss: 0.211630 lr: 0.001
2021-05-28 13:06:34 | INFO | Train Epoch: [18/30] [240/240], current batch Loss: 0.236493 lr: 0.001
2021-05-28 13:06:34 | INFO | Train Epoch: [19/30] [24/240], current batch Loss: 0.179534 lr: 0.001
2021-05-28 13:06:35 | INFO | Train Epoch: [19/30] [48/240], current batch Loss: 0.357148 lr: 0.001
2021-05-28 13:06:35 | INFO | Train Epoch: [19/30] [72/240], current batch Loss: 0.251600 lr: 0.001
2021-05-28 13:06:36 | INFO | Train Epoch: [19/30] [96/240], current batch Loss: 0.164827 lr: 0.001
2021-05-28 13:06:36 | INFO | Train Epoch: [19/30] [120/240], current batch Loss: 0.226071 lr: 0.001
2021-05-28 13:06:37 | INFO | Train Epoch: [19/30] [144/240], current batch Loss: 0.262766 lr: 0.001
2021-05-28 13:06:37 | INFO | Train Epoch: [19/30] [168/240], current batch Loss: 0.241002 lr: 0.001
2021-05-28 13:06:38 | INFO | Train Epoch: [19/30] [192/240], current batch Loss: 0.244567 lr: 0.001
2021-05-28 13:06:38 | INFO | Train Epoch: [19/30] [216/240], current batch Loss: 0.208930 lr: 0.001
2021-05-28 13:06:39 | INFO | Train Epoch: [19/30] [240/240], current batch Loss: 0.264355 lr: 0.001
2021-05-28 13:06:40 | INFO | val loss:0.2304659366607666, acc: 98.33333129882813
2021-05-28 13:06:40 | INFO | Train Epoch: [20/30] [24/240], current batch Loss: 0.211101 lr: 0.001
2021-05-28 13:06:41 | INFO | Train Epoch: [20/30] [48/240], current batch Loss: 0.222888 lr: 0.001
2021-05-28 13:06:41 | INFO | Train Epoch: [20/30] [72/240], current batch Loss: 0.157134 lr: 0.001
2021-05-28 13:06:42 | INFO | Train Epoch: [20/30] [96/240], current batch Loss: 0.176332 lr: 0.001
2021-05-28 13:06:42 | INFO | Train Epoch: [20/30] [120/240], current batch Loss: 0.295157 lr: 0.001
2021-05-28 13:06:43 | INFO | Train Epoch: [20/30] [144/240], current batch Loss: 0.288392 lr: 0.001
2021-05-28 13:06:43 | INFO | Train Epoch: [20/30] [168/240], current batch Loss: 0.230372 lr: 0.001
2021-05-28 13:06:44 | INFO | Train Epoch: [20/30] [192/240], current batch Loss: 0.223309 lr: 0.001
2021-05-28 13:06:44 | INFO | Train Epoch: [20/30] [216/240], current batch Loss: 0.260449 lr: 0.001
2021-05-28 13:06:45 | INFO | Train Epoch: [20/30] [240/240], current batch Loss: 0.174008 lr: 0.001
2021-05-28 13:06:45 | INFO | Train Epoch: [21/30] [24/240], current batch Loss: 0.208116 lr: 0.001
2021-05-28 13:06:46 | INFO | Train Epoch: [21/30] [48/240], current batch Loss: 0.273642 lr: 0.001
2021-05-28 13:06:47 | INFO | Train Epoch: [21/30] [72/240], current batch Loss: 0.155081 lr: 0.001
2021-05-28 13:06:47 | INFO | Train Epoch: [21/30] [96/240], current batch Loss: 0.141635 lr: 0.001
2021-05-28 13:06:48 | INFO | Train Epoch: [21/30] [120/240], current batch Loss: 0.173059 lr: 0.001
2021-05-28 13:06:48 | INFO | Train Epoch: [21/30] [144/240], current batch Loss: 0.162785 lr: 0.001
2021-05-28 13:06:49 | INFO | Train Epoch: [21/30] [168/240], current batch Loss: 0.327332 lr: 0.001
2021-05-28 13:06:49 | INFO | Train Epoch: [21/30] [192/240], current batch Loss: 0.161224 lr: 0.001
2021-05-28 13:06:50 | INFO | Train Epoch: [21/30] [216/240], current batch Loss: 0.196946 lr: 0.001
2021-05-28 13:06:50 | INFO | Train Epoch: [21/30] [240/240], current batch Loss: 0.207700 lr: 0.001
2021-05-28 13:06:51 | INFO | val loss:0.20418614149093628, acc: 98.33333129882813
2021-05-28 13:06:52 | INFO | Train Epoch: [22/30] [24/240], current batch Loss: 0.193149 lr: 0.001
2021-05-28 13:06:52 | INFO | Train Epoch: [22/30] [48/240], current batch Loss: 0.205216 lr: 0.001
2021-05-28 13:06:53 | INFO | Train Epoch: [22/30] [72/240], current batch Loss: 0.227834 lr: 0.001
2021-05-28 13:06:53 | INFO | Train Epoch: [22/30] [96/240], current batch Loss: 0.222633 lr: 0.001
2021-05-28 13:06:54 | INFO | Train Epoch: [22/30] [120/240], current batch Loss: 0.170252 lr: 0.001
2021-05-28 13:06:55 | INFO | Train Epoch: [22/30] [144/240], current batch Loss: 0.130732 lr: 0.001
2021-05-28 13:06:55 | INFO | Train Epoch: [22/30] [168/240], current batch Loss: 0.172227 lr: 0.001
2021-05-28 13:06:56 | INFO | Train Epoch: [22/30] [192/240], current batch Loss: 0.194638 lr: 0.001
2021-05-28 13:06:56 | INFO | Train Epoch: [22/30] [216/240], current batch Loss: 0.290496 lr: 0.001
2021-05-28 13:06:57 | INFO | Train Epoch: [22/30] [240/240], current batch Loss: 0.199986 lr: 0.001
2021-05-28 13:06:57 | INFO | Train Epoch: [23/30] [24/240], current batch Loss: 0.258152 lr: 0.001
2021-05-28 13:06:58 | INFO | Train Epoch: [23/30] [48/240], current batch Loss: 0.240600 lr: 0.001
2021-05-28 13:06:58 | INFO | Train Epoch: [23/30] [72/240], current batch Loss: 0.212154 lr: 0.001
2021-05-28 13:06:59 | INFO | Train Epoch: [23/30] [96/240], current batch Loss: 0.204619 lr: 0.001
2021-05-28 13:06:59 | INFO | Train Epoch: [23/30] [120/240], current batch Loss: 0.178061 lr: 0.001
2021-05-28 13:07:00 | INFO | Train Epoch: [23/30] [144/240], current batch Loss: 0.169702 lr: 0.001
2021-05-28 13:07:00 | INFO | Train Epoch: [23/30] [168/240], current batch Loss: 0.185973 lr: 0.001
2021-05-28 13:07:01 | INFO | Train Epoch: [23/30] [192/240], current batch Loss: 0.126774 lr: 0.001
2021-05-28 13:07:01 | INFO | Train Epoch: [23/30] [216/240], current batch Loss: 0.176147 lr: 0.001
2021-05-28 13:07:02 | INFO | Train Epoch: [23/30] [240/240], current batch Loss: 0.165644 lr: 0.001
2021-05-28 13:07:03 | INFO | val loss:0.1851447731256485, acc: 98.33333129882813
2021-05-28 13:07:03 | INFO | Train Epoch: [24/30] [24/240], current batch Loss: 0.101937 lr: 0.001
2021-05-28 13:07:04 | INFO | Train Epoch: [24/30] [48/240], current batch Loss: 0.228260 lr: 0.001
2021-05-28 13:07:04 | INFO | Train Epoch: [24/30] [72/240], current batch Loss: 0.172320 lr: 0.001
2021-05-28 13:07:05 | INFO | Train Epoch: [24/30] [96/240], current batch Loss: 0.181245 lr: 0.001
2021-05-28 13:07:05 | INFO | Train Epoch: [24/30] [120/240], current batch Loss: 0.213367 lr: 0.001
2021-05-28 13:07:06 | INFO | Train Epoch: [24/30] [144/240], current batch Loss: 0.153174 lr: 0.001
2021-05-28 13:07:06 | INFO | Train Epoch: [24/30] [168/240], current batch Loss: 0.147938 lr: 0.001
2021-05-28 13:07:07 | INFO | Train Epoch: [24/30] [192/240], current batch Loss: 0.175936 lr: 0.001
2021-05-28 13:07:07 | INFO | Train Epoch: [24/30] [216/240], current batch Loss: 0.207366 lr: 0.001
2021-05-28 13:07:08 | INFO | Train Epoch: [24/30] [240/240], current batch Loss: 0.125042 lr: 0.001
2021-05-28 13:07:09 | INFO | Train Epoch: [25/30] [24/240], current batch Loss: 0.164083 lr: 0.001
2021-05-28 13:07:09 | INFO | Train Epoch: [25/30] [48/240], current batch Loss: 0.241398 lr: 0.001
2021-05-28 13:07:10 | INFO | Train Epoch: [25/30] [72/240], current batch Loss: 0.218719 lr: 0.001
2021-05-28 13:07:10 | INFO | Train Epoch: [25/30] [96/240], current batch Loss: 0.146496 lr: 0.001
2021-05-28 13:07:11 | INFO | Train Epoch: [25/30] [120/240], current batch Loss: 0.070324 lr: 0.001
2021-05-28 13:07:12 | INFO | Train Epoch: [25/30] [144/240], current batch Loss: 0.149394 lr: 0.001
2021-05-28 13:07:12 | INFO | Train Epoch: [25/30] [168/240], current batch Loss: 0.235920 lr: 0.001
2021-05-28 13:07:13 | INFO | Train Epoch: [25/30] [192/240], current batch Loss: 0.142173 lr: 0.001
2021-05-28 13:07:13 | INFO | Train Epoch: [25/30] [216/240], current batch Loss: 0.152573 lr: 0.001
2021-05-28 13:07:14 | INFO | Train Epoch: [25/30] [240/240], current batch Loss: 0.176650 lr: 0.001
2021-05-28 13:07:14 | INFO | val loss:0.1685771256685257, acc: 98.33333129882813
2021-05-28 13:07:15 | INFO | Train Epoch: [26/30] [24/240], current batch Loss: 0.175324 lr: 0.001
2021-05-28 13:07:15 | INFO | Train Epoch: [26/30] [48/240], current batch Loss: 0.142630 lr: 0.001
2021-05-28 13:07:16 | INFO | Train Epoch: [26/30] [72/240], current batch Loss: 0.110943 lr: 0.001
2021-05-28 13:07:17 | INFO | Train Epoch: [26/30] [96/240], current batch Loss: 0.155159 lr: 0.001
2021-05-28 13:07:17 | INFO | Train Epoch: [26/30] [120/240], current batch Loss: 0.196372 lr: 0.001
2021-05-28 13:07:18 | INFO | Train Epoch: [26/30] [144/240], current batch Loss: 0.155243 lr: 0.001
2021-05-28 13:07:18 | INFO | Train Epoch: [26/30] [168/240], current batch Loss: 0.160808 lr: 0.001
2021-05-28 13:07:19 | INFO | Train Epoch: [26/30] [192/240], current batch Loss: 0.146129 lr: 0.001
2021-05-28 13:07:19 | INFO | Train Epoch: [26/30] [216/240], current batch Loss: 0.197075 lr: 0.001
2021-05-28 13:07:20 | INFO | Train Epoch: [26/30] [240/240], current batch Loss: 0.118990 lr: 0.001
2021-05-28 13:07:20 | INFO | Train Epoch: [27/30] [24/240], current batch Loss: 0.153846 lr: 0.001
2021-05-28 13:07:21 | INFO | Train Epoch: [27/30] [48/240], current batch Loss: 0.162352 lr: 0.001
2021-05-28 13:07:22 | INFO | Train Epoch: [27/30] [72/240], current batch Loss: 0.098237 lr: 0.001
2021-05-28 13:07:22 | INFO | Train Epoch: [27/30] [96/240], current batch Loss: 0.110784 lr: 0.001
2021-05-28 13:07:23 | INFO | Train Epoch: [27/30] [120/240], current batch Loss: 0.284921 lr: 0.001
2021-05-28 13:07:23 | INFO | Train Epoch: [27/30] [144/240], current batch Loss: 0.123655 lr: 0.001
2021-05-28 13:07:24 | INFO | Train Epoch: [27/30] [168/240], current batch Loss: 0.082322 lr: 0.001
2021-05-28 13:07:24 | INFO | Train Epoch: [27/30] [192/240], current batch Loss: 0.167122 lr: 0.001
2021-05-28 13:07:25 | INFO | Train Epoch: [27/30] [216/240], current batch Loss: 0.187331 lr: 0.001
2021-05-28 13:07:25 | INFO | Train Epoch: [27/30] [240/240], current batch Loss: 0.194329 lr: 0.001
2021-05-28 13:07:26 | INFO | val loss:0.15470733493566513, acc: 98.33333129882813
2021-05-28 13:07:27 | INFO | Train Epoch: [28/30] [24/240], current batch Loss: 0.114045 lr: 0.001
2021-05-28 13:07:27 | INFO | Train Epoch: [28/30] [48/240], current batch Loss: 0.133650 lr: 0.001
2021-05-28 13:07:28 | INFO | Train Epoch: [28/30] [72/240], current batch Loss: 0.134014 lr: 0.001
2021-05-28 13:07:29 | INFO | Train Epoch: [28/30] [96/240], current batch Loss: 0.098552 lr: 0.001
2021-05-28 13:07:29 | INFO | Train Epoch: [28/30] [120/240], current batch Loss: 0.128556 lr: 0.001
2021-05-28 13:07:30 | INFO | Train Epoch: [28/30] [144/240], current batch Loss: 0.083167 lr: 0.001
2021-05-28 13:07:30 | INFO | Train Epoch: [28/30] [168/240], current batch Loss: 0.169231 lr: 0.001
2021-05-28 13:07:31 | INFO | Train Epoch: [28/30] [192/240], current batch Loss: 0.136757 lr: 0.001
2021-05-28 13:07:31 | INFO | Train Epoch: [28/30] [216/240], current batch Loss: 0.176027 lr: 0.001
2021-05-28 13:07:32 | INFO | Train Epoch: [28/30] [240/240], current batch Loss: 0.122638 lr: 0.001
2021-05-28 13:07:32 | INFO | Train Epoch: [29/30] [24/240], current batch Loss: 0.115294 lr: 0.001
2021-05-28 13:07:33 | INFO | Train Epoch: [29/30] [48/240], current batch Loss: 0.141442 lr: 0.001
2021-05-28 13:07:33 | INFO | Train Epoch: [29/30] [72/240], current batch Loss: 0.149762 lr: 0.001
2021-05-28 13:07:34 | INFO | Train Epoch: [29/30] [96/240], current batch Loss: 0.129384 lr: 0.001
2021-05-28 13:07:34 | INFO | Train Epoch: [29/30] [120/240], current batch Loss: 0.115347 lr: 0.001
2021-05-28 13:07:35 | INFO | Train Epoch: [29/30] [144/240], current batch Loss: 0.138238 lr: 0.001
2021-05-28 13:07:36 | INFO | Train Epoch: [29/30] [168/240], current batch Loss: 0.168794 lr: 0.001
2021-05-28 13:07:36 | INFO | Train Epoch: [29/30] [192/240], current batch Loss: 0.138804 lr: 0.001
2021-05-28 13:07:36 | INFO | Train Epoch: [29/30] [216/240], current batch Loss: 0.138899 lr: 0.001
2021-05-28 13:07:37 | INFO | Train Epoch: [29/30] [240/240], current batch Loss: 0.127890 lr: 0.001
2021-05-28 13:07:38 | INFO | val loss:0.14180715829133989, acc: 98.33333129882813
